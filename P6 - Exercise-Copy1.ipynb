{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Project 6: Analyzing Stock Sentiment from Twits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "\n",
    "<Story>  \n",
    "    There is \n",
    "Each problem consists of a function to implement and instructions on how to implement the function. The parts of the function that need to be implemented are marked with a # TODO comment.Your code will be checked for the correct solution when you submit it to Udacity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "When you implement the functions, you'll only need to you use the packages you've used in the classroom, like Torch, NLTK. These packages will be imported for you. We recommend you don't add any import statements, otherwise the grader might not be able to run your code.\n",
    "\n",
    "The other packages that we're importing are project_helper and project_tests. These are custom packages built to help you solve the problems. The project_helper module contains utility functions and graph functions. The project_tests contains the unit tests for all the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "You've been considering using sentiment around specific stocks in your models. You've been subscribed to StockTwits for a while to stay up to date on trading news. You can collect these twits (similar to tweets) and now you want to build a model that can predict the sentiment of the text in the twits. Many of the existing models perform feature extraction manually, basically assigning sentiment scores to individual words by hand. Instead you'd like to train a neural network to learn the features itself then have it predict sentiment. This means you'll need labeled data.\n",
    "\n",
    "You collected a bunch of twits, then hand labeled the sentiment of each with the help of some interns. You wanted to capture the degree of sentiment so you decided to use a five-point scale: very negative, negative, neutral, positive, very positive. Each tweet is labeled -2 to 2 in steps of 1, from very negative to very positive. \n",
    "\n",
    "Here then, you'll build a sentiment analysis model that will learn to assign sentiment to tweets on its own, using this labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Load in the twits. This is a JSON object with structure like so:\n",
    "\n",
    "```\n",
    "{'data':\n",
    "  {'message_body': 'Tweet body text here',\n",
    "   'sentiment': 0},\n",
    "  {'message_body': 'Happy tweet body text here',\n",
    "   'sentiment': 1},\n",
    "   ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Twits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('twits.json', 'r') as f:\n",
    "    twits = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fields in our individual tweets:\n",
    "\n",
    "* `'message_body'`: The actual text in the tweet\n",
    "* `'sentiment'`: Score on the sentiment of the tweet, ranges from -2 to 2 in steps of 1, with 0 being neutral\n",
    "\n",
    "Remember that we want our network to look at some text and predict the sentiment. Our training input will be the message bodies, and we can use the sentiment score as training label for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data \n",
    "\n",
    "To see what tweets look like, let's load 10 tweets from the list. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'message_body': 'RT @google Our annual look at the year in Google blogging (and beyond) http://t.co/sptHOAh8 $GOOG',\n",
       "  'sentiment': 0,\n",
       "  'timestamp': '2012-01-01T00:06:01Z'},\n",
       " {'message_body': '$GOOG http://stks.co/1jQs Many market leaders appear extended. Are these moves sustainable? I think not.',\n",
       "  'sentiment': -1,\n",
       "  'timestamp': '2012-01-01T00:18:17Z'},\n",
       " {'message_body': '\"Deconstructing A Trade: $AAPL 12/29/2011\"-New Blog Post.  Yeah it\\'s New Year\\'s Eve but I\\'m married with kids http://t.co/6VV31tBY $STUDY',\n",
       "  'sentiment': 0,\n",
       "  'timestamp': '2012-01-01T00:26:18Z'},\n",
       " {'message_body': 'My prediction for 2012 is that the $spx and $djia ($spy and $dia) will make all time highs. And $aapl right along with them.',\n",
       "  'sentiment': 2,\n",
       "  'timestamp': '2012-01-01T00:30:36Z'},\n",
       " {'message_body': 'RT @bclund &quot;Deconstructing A Trade: $AAPL 12/29/2011&quot;New Blog Post. Yeah it&#39;s NY&#39;s Eve but I&#39;m married with kids http://stks.co/1jQy $STUDY',\n",
       "  'sentiment': 0,\n",
       "  'timestamp': '2012-01-01T00:34:52Z'},\n",
       " {'message_body': 'RT @bclund: \"Deconstructing A Trade: $AAPL 12/29/2011\"-New Blog Post. http://t.co/aJGevn88 $STUDY // Sick post. pure gold. 8 beers already?',\n",
       "  'sentiment': 0,\n",
       "  'timestamp': '2012-01-01T00:49:22Z'},\n",
       " {'message_body': 'Ten Great New Yearâ€™s Resolutions for Traders http://stks.co/1jRA  $study These resolutions apply whether you are trading $SPY or $AAPL',\n",
       "  'sentiment': 0,\n",
       "  'timestamp': '2012-01-01T00:59:12Z'},\n",
       " {'message_body': '@howardlindzon Funny check all your contacts too... use this list \"other contacts\" to see who google+ pushed in. $GOOG',\n",
       "  'sentiment': 1,\n",
       "  'timestamp': '2012-01-01T01:35:57Z'},\n",
       " {'message_body': 'Bank CEOs Earn Big Bucks Even as Stocks Get Slammed\\n http://t.co/R9TxBohX $JPM $GS $BAC',\n",
       "  'sentiment': 0,\n",
       "  'timestamp': '2012-01-01T01:56:16Z'},\n",
       " {'message_body': 'Prices are LOCKED on our 10 buy-and-holds for 2012 - $AA $ARCO $CAT $COF $FDX $HSY $MAKO $MSFT $STD $TKC http://stks.co/1jRN #beststocks',\n",
       "  'sentiment': 2,\n",
       "  'timestamp': '2012-01-01T02:22:38Z'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"loading 10 tweets from the list\"\"\"\n",
    "# TODO Implement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of data \n",
    "Now let's look at the number of twits in dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"print out the number of twits\"\"\"\n",
    "# TODO Implement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "If you get it right, we have 3 million tweets over all. For development purposes, let's only use the first 2 million tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"print out the first 2 million tweets\"\"\"\n",
    "# TODO Implement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is the data we'll train & test on\n",
    "messages = [twit['message_body'] for twit in data]\n",
    "# Adding 2 here to scale the sentiments to 0 to 4 for use in our network\n",
    "sentiments = [twit['sentiment'] + 2 for twit in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data pre-processing\n",
    "\n",
    "\n",
    "With our data in hand we need to preprocess our text. These tweets are collected by filtering on ticker symbols where these are denoted with a leader $ symbol in the tweet itself. For example,\n",
    "\n",
    "`{'message_body': 'RT @google Our annual look at the year in Google blogging (and beyond) http://t.co/sptHOAh8 $GOOG',\n",
    " 'sentiment': 0}`\n",
    "\n",
    "\n",
    "The ticker symbols don't provide information on the sentiment, and they are in every tweet, so we should remove them. This tweet also has the `@google` username, again not providing sentiment information, so we should also remove it. And, we see a URL `http://t.co/sptHOAh8`, let's remove these too.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to remove specific words or phrases is with regex, the `re` module. You can sub out specific patterns with a space:\n",
    "\n",
    "```python\n",
    "re.sub(pattern, ' ', text)\n",
    "```\n",
    "This will substitute a space with anywhere the pattern matches in the text. Later when we tokenize the text, we'll split appropriately on those spaces.\n",
    "\n",
    "Steps for pre-processing include: \n",
    "<img src=\"image3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pbarekatain/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Add this part to project helper \n",
    "\n",
    "nltk.download('wordnet')\n",
    "wnl = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(message):\n",
    "    ''' This function takes a string as input, then performs these operations: \n",
    "        * lowercase\n",
    "        * remove URLs\n",
    "        * remove ticker symbols \n",
    "        * removes punctuation\n",
    "        * tokenize by spliting the string on whitespace \n",
    "        * removes any single character tokens\n",
    "    ''' \n",
    "    #TODO: Implement \n",
    "    \n",
    "    # Lowercase \n",
    "    text = pass \n",
    "    \n",
    "    # Match and remove URLs\n",
    "    text = pass \n",
    "    \n",
    "    # Match and remove ticker symbols that start with $\n",
    "    text = pass  \n",
    "    \n",
    "    # Match and remove twitter usernames that start with @\n",
    "    text = pass  \n",
    "\n",
    "    # Replace punctuation and numbers (anything not a letter) with spaces\n",
    "    text = pass \n",
    "    \n",
    "    # Tokenize by splitting the string on whitespace\n",
    "    tokens = pass \n",
    "\n",
    "    # Lemmatize and remove any tokens with only one character\n",
    "    tokens = pass  \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess All the Twits \n",
    "Now we can preprocess each of the twits in our dataset. This will take a while since we have millions of twits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement \n",
    "tokenized = pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the Vocabulary\n",
    "\n",
    "Now with all of our messages tokenized, we want to create a vocabulary and count up how often each word appears in our entire corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement \n",
    "\n",
    "    \"\"\"\n",
    "    Create a vocabulary by using Bag of words\n",
    "    Use for loop to update your tokens\n",
    "    \n",
    "    \"\"\"\n",
    "bow = pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove common and rare words\n",
    "\n",
    "With our vocabulary, now we'll remove some of the most common words such as 'the', 'and', 'it', etc. These words don't contribute to identfying sentiment and are really common, resulting in a lot of noise in our input. If we can filter these out, then our network should have an easier time learning. It's up to you to decide how many to remove.\n",
    "\n",
    "We also want to remove really rare words that show up in a only a few tweets. Here you'll want to divide the count of each word by the number of messages. Then remove words that only appear in some small fraction of the messages. Again, it's up to you how much you want to keep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Frequency of Words Appearing in Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quot', 'for', 'on', 'the', 'and', 'of', 'in', 'is', 'it', 'to'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22810"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        freqs: frequency of words appearing in messages\n",
    "        low_cutoff: here we assign frequency cutoff to 0.00005, this is probably better as a percenet of the low frequency words. \n",
    "        high_cutoff: set a number for high frequency words instead of a frequency threshold. \n",
    "        * The distribution of word counts is peaked at the most frequent words with a really long tail, so it's usually better just cut off the first K words.\n",
    "        K_most_common \n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        print the K most common words in the vocab\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        # Frequency of words appearing in messages\n",
    "        freqs = pass \n",
    "        \n",
    "        # Frequency cutoff \n",
    "        low_cutoff = pass\n",
    "        \n",
    "        # K high frequency words \n",
    "        high_cutoff = pass \n",
    "        \n",
    "        K_most_common = pass \n",
    "\n",
    "\n",
    "        print(K_most_common)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering High and Low Frequency Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO Implement \n",
    "\n",
    "        \"\"\"\n",
    "        Filter high and low frequency words.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Filtered words \n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        Length of filtered words \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        filtered_words = pass \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updaing Vocabulary by Removing Filtered Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    " #TODO Implement\n",
    "    \"\"\"Go through all the data and remove words that aren't in our vocab\"\"\"\n",
    "        \n",
    "        vocab = pass \n",
    "        id2vocab = pass\n",
    "        filtered = pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the classes\n",
    "\n",
    "Let's do a few last pre-processing steps. If we look at how our tweets are labeled, we'll find that 50% of them are neutral. This means that our network will be 50% accurate just by guessing 0 every single time. To help our network learn appropriately, we'll want to balance our classes.\n",
    "That is, make sure each of our different sentiment scores show up roughly as frequently in the data.\n",
    "\n",
    "What we can do here is go through each of our examples and randomly drop tweets with neutral sentiment. What should be the probability we drop these tweets if we want to get around 20% neutral tweets starting at 50% neutral? We should also take this opportunity to remove messages with length 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#+++  Add Explaintion here \n",
    "n_neutral = sum(1 for each in sentiments if each == 2)\n",
    "N_examples = len(sentiments)\n",
    "keep_prob = (N_examples - n_neutral)/4/n_neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #TODO Implement\n",
    " #+++ Add more Explaintion here - see the solution notebook\n",
    "balanced = pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did it correctly, you should see the following result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1976361248285764"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#+++ Add this to project helper \n",
    "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
    "N_examples = len(balanced['sentiments'])\n",
    "n_neutral/N_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's convert our tokens into integer ids which we can pass to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
    "sentiments = balanced['sentiments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Now we have our vocabulary which means we can transform our tokens into ids, which are then passed to our network. So, let's define the network now!\n",
    "\n",
    "Here is a nice diagram showing the network we'd like to build: \n",
    "#### Embed -> RNN -> Dense -> Softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the text classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@+++ Before we build text classifier, if you remember from the network you built in  \"Sentiment Analysis with an RNN exercise\",  the network \" SentimentRNN\", here we named it \"TextClassifer\", consist of three main part: 1) init function `__init__` 2) forward pass `forward`  3) hidden state `init_hidden`. \n",
    "\n",
    "This network is pretty similar to the network you built, expect, in  `forward` pass, we use softmax instead of sigmoid. The reason we are not using sigmoid is because the output of NN is not a binary. In our network, sentiment scores have 5 possible outcomes. We are looking for an outcome with the highest probability thus softmax is a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement \n",
    "    \"\"\"\n",
    "     1. Define __init__  \n",
    "     2. Define forward  \n",
    "     3. Define init_hidden\n",
    "     \n",
    "    \"\"\"\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "     \n",
    "\n",
    "#  __init__ function    \n",
    "   \n",
    "    \"\"\"\n",
    "    Initialize the model by setting up the layers.\n",
    "    \n",
    "        Parameters\n",
    "        ---------- \n",
    "        Use the following parameters as the arguments for __init__ function: \n",
    "        (self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1)\n",
    "        - You can add more layers or change the dropout rate. \n",
    "                \n",
    "    \"\"\"\n",
    "    \n",
    " # forward pass \n",
    "    \n",
    "      \"\"\"\n",
    "      Perform a forward pass of our model on some input and hidden state.\n",
    "      \n",
    "        Parameters\n",
    "        ----------\n",
    "              Use softmax instead of sigmoid\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            return last softmax 'Logps' output and hidden state \n",
    "           \n",
    "    \"\"\" \n",
    "    \n",
    "      \n",
    "# hidden state \n",
    "    \n",
    "    \"\"\" \n",
    "    Initializes hidden state \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "                - Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "                - Initialized to zero, for hidden state and cell state of LSTM\n",
    "        Returns\n",
    "        -------\n",
    "            hidden \n",
    "    \"\"\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### View Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7422, -1.4971, -1.8657, -1.7451, -1.3031],\n",
      "        [-1.6957, -1.4752, -1.8318, -1.4041, -1.7033],\n",
      "        [-1.7506, -1.7483, -1.8730, -1.4560, -1.3264],\n",
      "        [-1.8018, -1.5647, -1.9210, -1.4892, -1.3711]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "#+++ Add this to project helper \n",
    "model = TextClassifier(len(vocab), 10, 6, 5, dropout=0.1, lstm_layers=2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
    "hidden = model.init_hidden(4)\n",
    "\n",
    "logps, _ = model.forward(input, hidden)\n",
    "print(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders and Batching - Optional \n",
    "\n",
    "\n",
    "Now we should build a generator that we can use to loop through our data. It'll be more efficient if we can pass our sequences in as a batch, that is some number of sequences all at the same time. Our input tensors should look like `(sequence_length, batch_size)`. So if our sequences are 40 tokens long and we pass in 25 sequences, then we'd have an input size of `(40, 25)`.\n",
    "\n",
    "If we set our sequence length to 40, what do we do with messages that are more or less than 40 tokens? For messages with fewer than 40 tokens, we will pad the empty spots with zeros. We should be sure to **left** pad so that the RNN starts from nothing before going through the data. If the message has 20 tokens, then the first 20 spots of our 40 long sequence will be 0. If a message has more than 40 tokens, we'll just keep the first 40 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional TODO implement: Attempt to build a dataloader \n",
    "    \"\"\" \n",
    "    Parameters\n",
    "    ----------\n",
    "    define dataloader\n",
    "    Use the following parameters as the arguments for dataloader function:\n",
    "    (messages, labels, sequence_length=30, batch_size=32, shuffle=False)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    batch, label_tensor       \n",
    "           \n",
    "    \"\"\"\n",
    "\n",
    "    def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
    "    \n",
    "    #TODO Implement \n",
    "   \n",
    "    yield batch, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, and  Validation\n",
    "With our data in nice shape, we'll split it into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement \n",
    "\n",
    "     \"\"\"\n",
    "     split data into training and validation sets \n",
    "    \n",
    "         Parameters\n",
    "         ----------\n",
    "         token_ids for valid_split\n",
    "        \n",
    "     \"\"\"   \n",
    "    \n",
    "    valid_split = pass   \n",
    "    train_features = pass  \n",
    "    valid_features = pass\n",
    "\n",
    "    train_labels = pass \n",
    "    valid_labels = pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#+++ Project Helper \n",
    "\n",
    "text_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))\n",
    "model = TextClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\n",
    "hidden = model.init_hidden(64)\n",
    "logps, hidden = model.forward(text_batch, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TextClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
    "model.embedding.weight.data.uniform_(-1, 1)\n",
    "epochs = 5\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "print_every = 100\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Train loss: 1.218, Val. Loss: 1.072, Val Acc.: 59.000\n",
      "Train loss: 0.952, Val. Loss: 0.968, Val Acc.: 63.000\n",
      "Train loss: 0.867, Val. Loss: 0.912, Val Acc.: 66.000\n",
      "Train loss: 0.818, Val. Loss: 0.879, Val Acc.: 68.000\n",
      "Train loss: 0.784, Val. Loss: 0.868, Val Acc.: 68.000\n",
      "Train loss: 0.758, Val. Loss: 0.854, Val Acc.: 69.000\n",
      "Train loss: 0.749, Val. Loss: 0.834, Val Acc.: 70.000\n",
      "Train loss: 0.752, Val. Loss: 0.834, Val Acc.: 69.000\n",
      "Train loss: 0.737, Val. Loss: 0.817, Val Acc.: 70.000\n",
      "Train loss: 0.729, Val. Loss: 0.816, Val Acc.: 70.000\n",
      "Train loss: 0.712, Val. Loss: 0.803, Val Acc.: 70.000\n",
      "Train loss: 0.715, Val. Loss: 0.796, Val Acc.: 71.000\n",
      "Train loss: 0.716, Val. Loss: 0.799, Val Acc.: 70.000\n",
      "Train loss: 0.706, Val. Loss: 0.807, Val Acc.: 71.000\n",
      "Train loss: 0.705, Val. Loss: 0.802, Val Acc.: 71.000\n",
      "Train loss: 0.702, Val. Loss: 0.807, Val Acc.: 71.000\n",
      "Train loss: 0.697, Val. Loss: 0.799, Val Acc.: 71.000\n",
      "Train loss: 0.703, Val. Loss: 0.806, Val Acc.: 71.000\n",
      "Train loss: 0.696, Val. Loss: 0.788, Val Acc.: 71.000\n",
      "Train loss: 0.709, Val. Loss: 0.775, Val Acc.: 71.000\n",
      "Train loss: 0.698, Val. Loss: 0.781, Val Acc.: 71.000\n",
      "Train loss: 0.692, Val. Loss: 0.787, Val Acc.: 71.000\n",
      "Train loss: 0.684, Val. Loss: 0.786, Val Acc.: 71.000\n",
      "Train loss: 0.691, Val. Loss: 0.790, Val Acc.: 71.000\n",
      "Train loss: 0.684, Val. Loss: 0.790, Val Acc.: 71.000\n",
      "Train loss: 0.692, Val. Loss: 0.782, Val Acc.: 71.000\n",
      "Train loss: 0.678, Val. Loss: 0.791, Val Acc.: 72.000\n",
      "Train loss: 0.683, Val. Loss: 0.788, Val Acc.: 71.000\n",
      "Train loss: 0.689, Val. Loss: 0.777, Val Acc.: 71.000\n",
      "Train loss: 0.688, Val. Loss: 0.780, Val Acc.: 71.000\n",
      "Train loss: 0.691, Val. Loss: 0.777, Val Acc.: 71.000\n",
      "Train loss: 0.689, Val. Loss: 0.782, Val Acc.: 71.000\n",
      "Train loss: 0.681, Val. Loss: 0.770, Val Acc.: 72.000\n",
      "Train loss: 0.685, Val. Loss: 0.784, Val Acc.: 72.000\n",
      "Train loss: 0.685, Val. Loss: 0.783, Val Acc.: 71.000\n",
      "Train loss: 0.689, Val. Loss: 0.777, Val Acc.: 71.000\n",
      "Train loss: 0.680, Val. Loss: 0.770, Val Acc.: 72.000\n",
      "Train loss: 0.677, Val. Loss: 0.775, Val Acc.: 72.000\n",
      "Train loss: 0.678, Val. Loss: 0.770, Val Acc.: 72.000\n",
      "Train loss: 0.686, Val. Loss: 0.768, Val Acc.: 72.000\n",
      "Train loss: 0.679, Val. Loss: 0.776, Val Acc.: 71.000\n",
      "Train loss: 0.673, Val. Loss: 0.781, Val Acc.: 71.000\n",
      "Train loss: 0.685, Val. Loss: 0.771, Val Acc.: 72.000\n",
      "Starting epoch 2\n",
      "Train loss: 0.651, Val. Loss: 0.792, Val Acc.: 72.000\n",
      "Train loss: 0.663, Val. Loss: 0.784, Val Acc.: 72.000\n",
      "Train loss: 0.654, Val. Loss: 0.796, Val Acc.: 72.000\n",
      "Train loss: 0.654, Val. Loss: 0.789, Val Acc.: 72.000\n",
      "Train loss: 0.659, Val. Loss: 0.786, Val Acc.: 72.000\n",
      "Train loss: 0.660, Val. Loss: 0.779, Val Acc.: 72.000\n",
      "Train loss: 0.672, Val. Loss: 0.775, Val Acc.: 72.000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement \n",
    "## TODO: Train your model with dropout, and monitor the training progress with the validation loss and accuracy\n",
    "model.to(device)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Starting epoch {epoch+1}\")\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    # batch loop\n",
    "    for text_batch, labels in dataloader(train_features, train_labels, \n",
    "                                         batch_size=batch_size, sequence_length=20, shuffle=True):\n",
    "        steps += 1\n",
    "        \n",
    "        # We're not passing the hidden state between batches, so create a new one here -- every training step \n",
    "        \n",
    "        hidden = model.init_hidden(labels.shape[0])\n",
    "        \n",
    "        # if you use GPU\n",
    "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "        for each in hidden:\n",
    "            each.to(device)\n",
    "            \n",
    "        # zero accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the output from the model\n",
    "        logps, hidden = model(text_batch, hidden)\n",
    "        \n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # loss stats\n",
    "        if steps % print_every == 0:\n",
    "            model.eval()\n",
    "            val_steps = 0\n",
    "            val_loss, accuracy = 0, 0\n",
    "            for text_batch, labels in dataloader(valid_features, valid_labels, batch_size=batch_size):\n",
    "                val_steps += 1\n",
    "                # We're not passing the hidden state between batches, so create a new one here\n",
    "                \n",
    "                \n",
    "                hidden = model.init_hidden(labels.shape[0])\n",
    "                text_batch, labels = text_batch.to(device), labels.to(device)\n",
    "                for each in hidden:\n",
    "                    each.to(device)\n",
    "                    \n",
    "                logps, hidden = model(text_batch, hidden)\n",
    "                loss = criterion(logps, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                topval, topclass = torch.exp(logps).topk(1)\n",
    "                accuracy += torch.sum(topclass.squeeze() == labels)\n",
    "\n",
    "            print(f\"Train loss: {running_loss/print_every:.3f}, Val. Loss: {val_loss/val_steps:.3f}, Val Acc.: {100*accuracy/len(valid_labels):.3f}\")\n",
    "            model.train()\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Okay, now that you have a trained model, try it on some new tweets and see if it works appropriately. Remember that for any new text, you'll need to preprocess it first before passing it to the network. You should also think about how to handle input words that aren't in your vocabulary.\n",
    "\n",
    "We also want to use these sentiment scores in a larger ensemble model which you'll be learning about next. For this, we'll need the output to be some continuous value, typically on a scale of -3 to 3. Our model is predicting the probability on this discrete 5 value scale. Since we have a probability distribution, a good way to convert this to a continuous value is using the expectated value of the score. The expected value $\\bar{s}$ is the sum of each score $s_i$ multiplied by the probability $p_i$ of getting that score.\n",
    "\n",
    "$$\n",
    "\\large \\bar{s} = \\sum_i p_i s_i\n",
    "$$\n",
    "\n",
    "This is nice because it captures uncertainty in our model's predictions. For example, if it predicts 50% in positive ($s = 1$) and 50% in strongly positive ($s=2$), the expected value will be in between at 1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement \n",
    "\n",
    "    \"\"\" \n",
    "        Prints out whether a twit is predicted to be \n",
    "        positive or negative in sentiment, using a trained model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        - define predict function \n",
    "        text \n",
    "        model \n",
    "        vocab \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        expectation.item() \n",
    "    \"\"\"\n",
    "    \n",
    "    def predict(text, model, vocab): \n",
    "        \n",
    "    # TODO Implement  \n",
    "    tokens = preprocess(text)\n",
    "\n",
    "    \n",
    "    # Filter non-vocab words and convert to ids\n",
    "    tokens = pass \n",
    "    if len(tokens) == 0:\n",
    "        return None, None\n",
    "        \n",
    "    # Adding a batch dimension\n",
    "    text_input = pass \n",
    "    hidden = pass \n",
    "    logps, = pass \n",
    "    ps = pass \n",
    "\n",
    "    # Sentiment expectation\n",
    "    expectation = pass \n",
    "    \n",
    "    return expectation.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to project helper \n",
    "text = \"Good earnings this year, I'm bullish on $goog\"\n",
    "model.to(\"cpu\")\n",
    "predict(text, model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained model and we can make predictions. We can use this model to track the sentiments of various stocks by predicting the sentiments of twits as they are coming in. Now we have a stream of twits. For each of those twits, pull out the stocks mentioned in them and keep track of the sentiments. Remember that in the twits, ticker symbols are encoded with a dollar sign as the first character, all caps, and 2-4 letters, like $AAPL. Ideally, you'd want to track the sentiments of the stocks in your universe and use this as a signal in your larger model(s).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_twits.json', 'r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twit Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twit_stream():\n",
    "    for twit in test_data['data'][:1000]:\n",
    "        yield twit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(twit_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have voc, and you need create the steem of signals for the tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement \n",
    "\n",
    "\"\"\" Given a stream of twits and a universe of tickers, return sentiment scores for \n",
    "tickers in the universe. \"\"\"\n",
    "\n",
    "     \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        define score_twits \n",
    "        Use the following arguments for your function:\n",
    "        (stream, model, vocab, universe):\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score_twits \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def score_twits(stream, model, vocab, universe):\n",
    "   \n",
    "        for twit in stream:\n",
    "    \n",
    "    # Get the message_body of twits \n",
    "    text = pass\n",
    "    # use re.findall method in re \n",
    "    symbols = pass \n",
    "    score = pass \n",
    "        \n",
    "    for symbol in symbols:\n",
    "        if symbol in universe:\n",
    "        yield symbols, score, twit['timestamp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
    "score_stream = score_twits(twit_stream(), model, vocab, universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(score_streem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
